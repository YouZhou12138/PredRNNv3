Architecture: "PredFormer"

Seed: 27

Setting: taxibj

Logger:
    save_dir: "experiments/"

ckpt_path: None

Checkpointer:
    save_top_k: 1
    save_last: True
    every_n_epochs: 1

Trainer:
#    strategy: 'ddp'
    accelerator: 'gpu'
    log_every_n_steps: 64
    devices: [0]  # 改动
    max_epochs: 200
    gradient_clip_val: 1

Data:
    val_batch_size: 16
    test_batch_size: 16
    train_batch_size: 16
    num_workers: 4

Task:
    loss:
        name: "MSE"

    context_length: 4
    target_length: 4
    n_stochastic_preds: 1
    metric: "MSE"
    optimization:
        optimizer:
            - name: AdamW
              args:
                  betas: [ 0.9, 0.999 ]
              lr_per_sample: 0.00000625
        lr_shedule:
            - name: Cosine
              args:
                  t_initial: 200 #[2, 20, 50, 90]
                  lr_min: 0.000001

    n_log_batches: 2


Model:
    patch_size: 4
    num_channels: 2
    pre_seq: 4
    height: 32
    width: 32
    dim: 256
    dim_head: 32
    scale_dim: 4
    heads: 8
    dropout: 0.1
    attn_dropout: 0.1
    drop_path: 0.1
    depth: 1
    Ndepth: 4





